\documentclass{article}
\usepackage[latin9]{inputenc}
\usepackage{enumitem}
\usepackage{microtype}
\tolerance=2000
\hyphenpenalty=50
%%%%%%%%%
\usepackage{glossaries}

% Define your glossaries and acronyms here
\newglossaryentry{ai}{
	name=AI,
	description={Artificial Intelligence, the simulation of human intelligence in machines}
}

\newglossaryentry{ml}{
	name=ML,
	description={Machine Learning, a subset of AI that includes algorithms that enable machines to improve at tasks with experience}
}

\newglossaryentry{iot}{
	name=IoT,
	description={Internet of Things, the network of physical devices that are embedded with sensors, software, and other technologies for the purpose of connecting and exchanging data with other devices and systems over the Internet}
}

\newglossaryentry{fp}{
	name=FP,
	description={Floating Point, a method for representing real numbers that supports a wide range of values}
}

\newglossaryentry{sbs}{
	name=SBS,
	description={Spike-Based Systems, a type of neural network computation method}
}

\newglossaryentry{lif}{
	name=LIF,
	description={Leaky Integrate-and-Fire, a model used in spiking neural networks}
}

\newglossaryentry{mac}{
	name=MAC,
	description={Multiply-Accumulate, a common operation in digital signal processing and neural network computation}
}

\newglossaryentry{snn}{
	name=SNN,
	description={Spike Neural Network, a type of neural network computation method}
}

\newglossaryentry{cnn}{
	name=CNN,
	description={Convolutional Neural Network, a type of neural network computation method}
}

\newglossaryentry{hf6}{
	name=HF6,
	description={}
}

\newglossaryentry{tp}{
	name=TP,
	description={}
}

\newglossaryentry{qat}{
	name=QAT,
	description={}
}
\makeglossaries
%%%%%%%%%

\title{Yarib Israel Nevarez Esparza \\ Low-Power Neural Network Accelerators:
	Advancements in Custom Floating-Point Techniques}
\author{}
\date{\today}

\begin{document}

\maketitle

\title{\textbf{Abstract}}
\thispagestyle{empty}

The expansion of \gls{ai} is addressing a new era characterized by omnipresent connected devices. To ensure the sustainability of this transformation, it is imperative to adopt design strategies that harmonize precise computational results with economically viable system architectures. Consequently, refining the efficiency and quality of \gls{ai} hardware engines stands as a critical consideration in this evolution. This necessitates a balanced approach that prioritizes energy-efficient computations, precise and reliable results, and integration across various platforms and devices.

\gls{ml} algorithms are serving as the foundational enabler for the integration of \gls{ai} into \gls{iot} devices, particularly in the context of Industry 4.0. These advancements are shaping applications to be more intelligent and economically rewarding. This transformation improves numerous domains, from scientific research to industrial processes and everyday living. However, this technological evolution also brings its own set of challenges. \gls{ml} algorithms pose significant computational and energy demands. Consequently, a central objective of this dissertation is to explore innovative methods for enhancing the hardware efficiency of computing engines.

Approximate computing techniques, such as quantization, exploit the inherent error resilience of \gls{ml} algorithms to address key design concerns in computer systems: energy efficiency, performance, and chip area. Quantization, which involves reducing the number of bits used to represent numbers, can significantly lower power consumption and data movement, thereby enhancing energy efficiency by employing compact arithmetic units that save chip area. These techniques often yield computation acceleration due to reduced data sizes, which promotes faster, more parallel, and pipelined processing, particularly in neural network computation. However, this approach introduces a trade-off between precision and model accuracy, necessitating proper hardware design methodologies. While state-of-the-art methods are advancing, significant research opportunities remain, especially for accelerators with custom \gls{fp} computation.

In this dissertation, a hardware design methodology is presented for low-power inference of \gls{sbs} neural networks for embedded applications, within the field of \glspl{snn}. Compared to conventional \glspl{snn} employing the \gls{lif} mechanism, \gls{sbs} neural networks are highlighted for their reduced model complexity and exceptional noise robustness. However, despite their advantages, \gls{sbs} networks inherently possess a memory footprint and computational cost that makes them challenging for deployment in constrained devices. To solve this issue, this research leverages the intrinsic error resilience of \gls{sbs} models, aiming to enhance performance and reduce hardware complexity, while avoiding quantization. Specifically, this research introduces a novel \gls{mac} module designed to optimize the balance between computational accuracy and resource efficiency of \gls{fp} operations. This \gls{mac} module features configurable quality through a hybrid approach. It combines standard \gls{fp} number representations with a custom 8-bit \gls{fp} format, as well as a 4-bit logarithmic number representation. This design excludes the use of a sign bit, further contributing to the compact and efficient representation of numbers. This design enables the \gls{mac} module to be tailored to the specific resource constraints and performance requirements of a given application, making \gls{sbs} neural networks possible for deployment in resource-constrained environments.

In the field of \glspl{cnn}, this dissertation presents a hardware design methodology for low-power inference, specifically targeting sensor analytics applications. Central to this work is the proposal of the \gls{hf6} quantization scheme and its dedicated hardware accelerator, designed to function as a Conv2D \gls{tp}. This quantization strategy employs a hybrid number representation, combining standard \gls{fp} and a 6-bit \gls{fp} format. This strategy allows for a highly optimized \gls{fp} \gls{mac}, reducing mantissa multiplication into a multiplexer-adder operation. This research introduces a \gls{qat} method that, in certain cases, offers beneficial regularization effects. The efficacy of this exploration is demonstrated with a regression model, which improves its precision despite the applied quantization. For \gls{ml} portability, the custom \gls{fp} representation is encapsulated within a standard format -- a design characteristic that enables the proposed hardware to process it automatically. To validate the interoperability of this approach, the hardware architecture is integrated with TensorFlow Lite, demonstrating compatibility with industry-standard \gls{ml} frameworks and affirming the potential for practical deployment in various sensing applications while maintaining compliance with established \gls{ml} infrastructure.


This dissertation addresses an essential challenge in the current technological landscape: the harmonization of computational accuracy with energy efficiency and compatibility of hardware solutions. This dissertation stands as a significant contribution towards the development of a sustainable next-generation of neural network processors, essential to empower the increasingly connected and intelligent world of tomorrow.

\newpage
\title{\textbf{Kurzfassung}}

\thispagestyle{empty}

Die Ausweitung K\"unstlicher Intelligenz (KI) f\"uhrt in eine neue \"Ara, die von omnipr\"asent  vernetzten Ger\"aten gepr\"agt ist. Um die Nachhaltigkeit dieses Wandels zu gew\"ahrleisten, ist es unerl\"asslich, Designstrategien zu verfolgen, die pr\"azise Rechenergebnisse mit wirtschaftlich tragf\"ahigen Systemarchitekturen in Einklang bringen. Daher ist die Verfeinerung der Effizienz und Qualit\"at von KI-Hardware-Engines bei dieser Entwicklung von entscheidender Bedeutung. Dies erfordert einen ausgewogenen Ansatz, der die Energieeffizienz der Berechnungen, Pr\"azision und Zuverl\"assigkeit der Ergebnisse sowie die Integration \"uber verschiedene Plattformen und Ger\"ate hinweg priorisiert.

Machine Learning (ML)-Algorithmen dienen als grundlegende Voraussetzung f\"ur die Integration von KI in Ger\"ate des Internets der Dinge (IoT), insbesondere im Kontext von Industrie 4.0. Diese Weiterentwicklungen beeinflussen die Gestaltung von Anwendungen, die intelligenter und \"okonomisch vorteilhafter werden sollen. Dieser Wandel verbessert zahlreiche Bereiche, von der wissenschaftlichen Forschung \"uber industrielle Prozesse bis hin zum Alltag. Allerdings bringt diese technologische Entwicklung auch eigene Herausforderungen mit sich. ML-Algorithmen sind mit einem erheblichen Rechen- und Energiebedarf verbunden. Zentrales Ziel dieser Dissertation ist es daher, innovative Methoden zur Verbesserung der Hardwareeffizienz von Rechenmaschinen zu erforschen.

Approximative Rechentechniken, wie die Quantisierung, nutzen die inh\"arente Fehlerresistenz von ML-Algorithmen aus, um wichtige Designprobleme in Computersystemen anzugehen: die Energieeffizienz, die Leistung und die Chipfl\"ache. Durch Quantisierung, bei der die Anzahl der zur Darstellung von Zahlen verwendeten Bits reduziert wird, k\"onnen der Stromverbrauch und der Datenfluss erheblich reduziert und dadurch die Energieeffizienz verbessert werden, indem fl\"achensparendere Chips in kompakten Recheneinheiten eingesetzt werden. Diese Techniken f\"uhren h\"aufig zu einer Rechenbeschleunigung aufgrund reduzierter Datenpaketgr\"o\ss{}en. Dadurch wird eine schnellere, parallelere und in Pipelines ausgef\"uhrte Verarbeitung gef\"ordert, insbesondere bei der Berechnung neuronaler Netze. Andererseits f\"uhrt dieser Ansatz jedoch zu einem Kompromiss zwischen Zahlengenauigkeit und Modellgenauigkeit, was geeignete Methoden f\"ur den Hardwareentwurf erfordert. Besonders im Hinblick auf Beschleuniger mit benutzerdefinierter Gleitkommaberechnung (FP) gibt es trotz der Fortschritte bei den Methoden des Stands der Technik immer noch erheblichen Raum f\"ur weiterf\"uhrende Forschung.

In dieser Dissertation wird eine Hardware-Design-Methodik f\"ur Low-Power-Inferenz von neuronalen Spike-by-Spike (SbS)-Netzen f\"ur eingebettete Anwendungen im Bereich der Spiking Neural Networks (SNNs) vorgestellt. Im Vergleich zu herk\"ommlichen SNNs, die den Leaky Integrate-and-Fire (LIF)-Mechanismus verwenden, werden neuronale SbS-Netzwerke wegen ihrer reduzierten Modellkomplexit\"at und au\ss{}ergew\"ohnlichen Rauschrobustheit beleuchtet. Trotz ihrer Vorteile haben SbS-Netzwerke jedoch von Natur aus einen Speicherplatzbedarf und Rechenkosten, die den Einsatz in eingeschr\"ankten eingebetteten Systemen zu einer Herausforderung machen. Um dieses Problem zu l\"osen, verfolgt diese Forschungsarbeit die intrinsische Fehlerresilienz von SbS-Modellen zur Leistungsverbesserung und Reduktion der Hardwarekomplexit\"at bei gleichzeitiger Vermeidung von Zahlenquantisierung. Insbesondere f\"uhrt diese Forschungsarbeit ein neuartiges Multiply-Accumulate (MAC)-Modul ein, das entwickelt wurde, um das Gleichgewicht zwischen Rechengenauigkeit und Ressourceneffizienz von FP-Operationen zu optimieren. Dieses MAC-Modul bietet konfigurierbare Qualit\"at durch einen hybriden Ansatz. Es kombiniert Standard-FP-Zahlendarstellungen mit einem benutzerdefinierten 8-Bit-FP-Format sowie einer logarithmischen 4-Bit-Zahlendarstellung. Ferner kommt dieses Design ohne Verwendung eines Vorzeichenbits aus und tr\"agt somit weiter zur kompakten und effizienten Darstellung von Zahlen bei. Dar\"uber hinaus erm\"oglicht dieses Design, das MAC-Modul an die spezifischen Ressourcenbeschr\"ankungen und Leistungsanforderungen einer bestimmten Anwendung anzupassen, wodurch neuronale SbS-Netzwerke f\"ur den Einsatz in Umgebungen mit eingeschr\"ankten Ressourcen bereitgestellt werden k\"onnen.

Im Bereich der Convolutional Neural Networks (CNNs) stellt diese Dissertation eine Hardware-Design-Methodik f\"ur Low-Power-Inferenz vor, die speziell auf Sensor-Analyse-Anwendungen abzielt. Im Mittelpunkt dieser Arbeit steht der Vorschlag f\"ur das Quantisierungsschema Hybrid-Float6 (HF6) und sein dedizierter Hardwarebeschleuniger, der als Conv2D-Tensorprozessor (TP) fungieren soll. Diese Quantisierungsstrategie verwendet eine hybride Zahlendarstellung, welche Standard-FP mit einem 6-Bit-FP-Format kombiniert. Diese Strategie erm\"oglicht einen hochoptimierten FP-MAC, der die Mantissenmultiplikation auf eine Multiplexer-Addierer-Operation reduziert. Diese Forschungsarbeit f\"uhrt eine Quantization-Aware Training (QAT)-Methode ein, die in bestimmten F\"allen vorteilhafte Regularisierungseffekte bietet. Die Wirksamkeit dieses Ansatzes wird in einem Regressionsmodell demonstriert, das trotz der angewendeten Quantisierung eine verbesserte Genauigkeit zeigt. F\"ur die ML-Portabilit\"at wird die benutzerdefinierte FP-Darstellung in ein Standardformat gekapselt - ein Designmerkmal, das es der vorgeschlagenen Hardware erm\"oglicht, sie automatisch zu verarbeiten. Um die Interoperabilit\"at dieses Ansatzes zu validieren, wird die Hardware-Architektur in TensorFlow Lite integriert. Hiermit wird die Kompatibilit\"at zum Industriestandard-ML-Frameworks demonstriert und das Potenzial f\"ur den praktischen Einsatz in verschiedenen Sensoranwendungen unter Beibehaltung der Einhaltung der etablierten ML-Infrastruktur best\"atigt.

Diese Dissertation befasst sich mit einer wesentlichen Herausforderung in der aktuellen technologischen Landschaft: der Harmonisierung von Rechengenauigkeit mit Energieeffizienz und der Kompatibilit\"at von Hardwarel\"osungen. Sie leistet einen wesentlichen Beitrag zur Entwicklung einer nachhaltigen n\"achsten Generation von neuronalen Netzwerkprozessoren, die f\"ur die St\"arkung der zunehmend vernetzten und intelligenten Welt von morgen unerl\"asslich sind.

\end{document}

