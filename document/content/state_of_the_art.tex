\section{State of the art}
\subsection{Industrial Internet-of-Things}
Internet of Things (IoT) is a computing concept describing ubiquitous connection to the Internet, turning common objects into connected devices. The key idea behind the IoT concept is to deploy billions or even trillions of smart objects capable of sensing the surrounding environment, transmit and process acquired data, and then feedback to the environment. Connecting unconventional objects to the Internet will improve the sustainability and safety of industries and society, and enable efﬁcient interaction between the physical world and its digital counterpart, i.e., what is usually addressed as a cyber-physical system (CPS). IoT is usually depicted as the disruptive technology for solving most of present-day society issues such as smart cities, intelligent transportation, pollution monitoring, and connected healthcare, to name a few \cite{sisinni2018industrial}.

As a subset of IoT, Industrial IoT (IIoT) covers the domains of machine-to-machine (M2M) and industrial communication technologies with automation applications. IIoT paves the way for better understanding of the manufacturing process, thereby enabling efﬁcient and sustainable production. IIoT applications typically require relatively small throughput per node and the capacity is not a main concern. Instead, the need for connecting a very large number of devices to the Internet at low cost, with limited hardware capabilities and energy resources (e.g., small batteries) makes latency, energy efﬁciency, cost, reliability, and security/privacy more desired features \cite{aakerberg2011future}.

The scientific community has explored ideas for IIoT deployment architectures, as an example the Cloud of Things \cite{lin2015industrial,aazam2014cloud}. Where it considered direct communication between things and the Cloud, which is not real-time helpful. As an improved architecture, the Fog and Edge computing concepts provide computational resources closer to the industrial devices, mitigating some of the problems that cannot be addressed by the cloud \cite{bonomi2012fog,cisco2015fog}. Nevertheless, still with real-time difficulties.

Since most of IIoT will deal with real-time and concurrent scenarios requiring time synchronization, bringing real-time issues to the core of the problem. Also, reliability, predictability, robustness, and fault tolerance are necessary when dealing with mission critical systems \cite{de2018application}.

In order to enhance the real-time ML performance of IIoT devices, in this research, it is proposed the development of SoC architectures equipped with dedicated hardware acceleration for real-time ML computations.

\subsection{Machine learning accelerators}
Recently, ML algorithms have been successfully used to learn in a wide variety of applications, but their heavy computation demands have considerably limited their practical applications. These demands have led to arise in specialized hardware acceleration for ML.

The following paragraphs describe state of the art publications in ML hardware accelerators. We can find accelerators based on algorithm, morphologic, and also based on available hardware resources. The state of the art implementations are taken as references, for further research.

\subsubsection{DLAU: A scalable deep learning accelerator unit on FPGA}

A suitable example of accelerator unit is presented in \cite{wang2016dlau}, which presents the design of deep learning accelerator unit (DLAU), which is a scalable accelerator architecture for large-scale deep learning networks using ﬁeld-programmable gate array (FPGA) as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications. Experimental results on Xilinx FPGA prototype show that DLAU can achieve 36.1× speedup with reasonable hardware cost and low power utilization.

\subsubsection{Hybrid working set algorithm for SVM learning with a kernel coprocessor on FPGA}
Another example of hardware acceleration is presented in \cite{venkateshan2014hybrid}, which implements the model of Support vector machines (SVM). The associated compute intensive learning algorithm limits their use in real-time applications. In \cite{venkateshan2014hybrid} it is presented a fully scalable architecture of a coprocessor, which can compute multiple rows of the kernel matrix in parallel. Further, \cite{venkateshan2014hybrid} proposes an extended variant of the popular decomposition technique, sequential minimal optimization, which is called hybrid working set (HWS) algorithm, to effectively utilize the beneﬁts of cached kernel columns and the parallel computational power of a coprocessor. The coprocessor is implemented on Xilinx Virtex 7 ﬁeld-programmable gate array based VC707 board and achieves a speedup of upto 25× for kernel computation over single threaded computation on Intel Core i5. An application speedup of up to 15× over software implementation of LIBSVM and speedup of up to 23× over SVMLight is achieved using the HWS algorithm in unison with the coprocessor.

\subsubsection{DeepX: Deep learning accelerator for restricted boltzmann machine artificial neural networks}
In \cite{kim2017deepx}, Lok-Won Kim proposes a fully pipelined acceleration architecture to alleviate high computational demand of an artiﬁcial neural network (ANN) which is restricted Boltzmann machine (RBM) ANNs. The implemented RBM ANN accelerator (integrating 1024 × 1024 network size, using 128 input cases per batch, and running at a 303-MHz clock frequency) integrated in a state-of-the art ﬁeld-programmable gate array (FPGA) (Xilinx Virtex 7 XC7V-2000T) provides a computational performance of 301-billion connection-updates-per-second and about 193 times higher performance than a software solution running on general purpose processors. Most importantly, the architecture enables over 4 times (12 times in batch learning) higher performance compared with a previous work when both are implemented in an FPGA device (XC2VP70).

\subsubsection{A digital implementation of extreme learning machines for resource-constrained devices}
A particular example for resource constrained embedded systems, is given in \cite{ragusa2018digital}, which exhibit the implementation of single hidden-layer feed forward neural networks, based on hard-limit activation functions, on reconﬁgurable devices. The resulting design strategy relies on a novel learning procedure that inherits the approach adopted in the Extreme Learning Machine paradigm. The eventual training process balances accuracy and network complexity effectively, thus supporting a digital architecture that prioritizes area utilization over computational performance. Experimental veriﬁcations proved that the proposed design strategy supports the realization of embedded classiﬁers in both FPGA and low-end, inexpensive devices such as CPLDs.

\subsubsection{Simplifying deep neural networks for FPGA-like neuromorphic systems}
Brain-like hardware platforms for the brain-inspired computational models are being studied, but the maximum size of neural networks they can evaluate is often limited by the number of neurons and synapses equipped with the hardware. The \cite{chung2018simplifying} presents two techniques, factorization and pruning, that not only compress the models but also maintain the form of the models for the execution on neuromorphic architectures. The \cite{chung2018simplifying} also proposes a novel method to combine the two techniques. The proposed method shows signiﬁcant improvements in reducing the number of model parameters over standalone use of each method while maintaining the performance. Our experimental results show that the proposed method can achieve 30x reduction rate within $1\%$ budget of accuracy for the largest layer of AlexNet.

\pagebreak