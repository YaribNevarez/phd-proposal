%\section{Abstract}
\begin{abstract}
	Machine Learning (ML) algorithms represent a promising solution for delivering intelligence to the emerging era of Internet-of-Things (IoT). As the volume of data collected increases, applications in this field become more intelligent and profitable, driving the evolution of many aspects of daily life, science, and industry. However, ML algorithms, particularly Spiking Neural Networks (SNNs) and deep Convolutional Neural Networks (CNNs), are highly compute and data intensive. Therefore, the substantial demand for power and hardware resources of these algorithms represents a restriction for IoT devices in the scope of embedded systems.
	
	Considering the intrinsic error resilience of ML algorithms, paradigms such as approximate computing come to the rescue by offering promising efficiency gains, especially in terms of performance, power consumption, and resource utilization. Approximation techniques are widely used in ML algorithms at the model-structure as well as at the hardware processing level. However, state-of-the-art approximate computing methodologies do not sufficiently address accelerator designs for SNN and deep CNN as power- and resource-demanding algorithms.
	
	
	This PhD proposal focuses on the investigation of approximate computing techniques to exploit the intrinsic error tolerance of ML algorithms to optimize computing embedded systems at the hardware architecture and circuit-level to achieve efficiency gains. The goal of this research is to contribute to state-of-the-art knowledge of this domain with methodologies to address accelerator designs for SNNs and deep CNNs. Furthermore, the expected outcome of this work is to develop high-efficiency accelerator architectures for SNN and deep CNN algorithms for computer vision applications (e.g., real-time multiple object detection and classification). Lastly, this concludes to enhance machine learning processing capabilities for the rise of the next generation of IoT devices.
\end{abstract}