%\section{Abstract}
\begin{abstract}
	Machine Learning (ML) algorithms represent a promising solution for delivering intelligence to the emerging domain of Internet-of-Things (IoT). As the volume of data collected increases, applications in this field become more intelligent and profitable, driving the evolution of many aspects of daily life, science, and industry. However, ML algorithms, particularly Spiking Neural Networks (SNN) and deep Convolutional Neural Networks (CNN), are highly compute and data intensive. Therefore, the substantial demand for power and hardware resources of these algorithms represents a restriction for IoT devices in the area of embedded systems.
	
	Considering the intrinsic error resilience of ML algorithms, paradigms such as approximate computing come to the rescue by offering promising efficiency gains, especially in terms of performance, power consumption, and resource utilization. Approximation techniques are widely used in ML algorithms at the model-structure as well as at the hardware processing level. However, state-of-the-art approximate computing methodologies do not sufficiently address accelerator designs for SNN and deep CNN as resource-demanding algorithms.
	
	
	This PhD proposal focuses on the investigation of approximate computing techniques to exploit the intrinsic error tolerance of ML algorithms to optimize computing embedded systems at the hardware architecture and circuit-level to achieve efficiency gains. The goal of this research is to contribute to state-of-the-art knowledge of approximate computing methodologies to address accelerator designs for SNN and deep CNN. Finally, the expected outcome of this work is to develop high-efficiency accelerator architectures for the SNN and CNN algorithms and ultimately enhance machine learning processing capabilities for the rise of the next generation of IoT devices.
\end{abstract}